{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from interruptingcow import timeout\n",
    "\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from cdlib import algorithms, classes, evaluation, readwrite\n",
    "\n",
    "class SpotifyGraph():\n",
    "\n",
    "    def __init__(self, dir, features_dir):\n",
    "\n",
    "        self.base_dir = path.join(dir, \"dataset\")\n",
    "        self.save_dir = path.join(dir, \"results\")\n",
    "        self.tracks_pth = path.join(self.base_dir, \"tracks.json\")\n",
    "        self.col_pth = path.join(self.base_dir, \"collections.json\")\n",
    "        self.graph_pth = path.join(self.base_dir, \"graph.json\")\n",
    "\n",
    "        self.ft_dir = features_dir\n",
    "        self.features_dict = {}\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Loading graph...\")\n",
    "        # with open(self.tracks_pth, \"r\", encoding=\"utf-8\") as f:\n",
    "        #     self.tracks = json.load(f)\n",
    "        # with open(self.col_pth, \"r\", encoding=\"utf-8\") as f:\n",
    "        #     self.collections = json.load(f)\n",
    "        with open(self.graph_pth, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.graph = json.load(f)\n",
    "\n",
    "    def save_graph(self, G):\n",
    "        with open(path.join(self.base_dir, \"filtered_graph.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict(tracks=[n for n in G.nodes() if n in self.track_ids_deg.keys()],\n",
    "                           collections=[n for n in G.nodes() if n in self.col_ids_deg.keys()],\n",
    "                           edges=[{\"from\" : u, \"to\" : v} for u,v in G.edges()]),\n",
    "                           f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def to_nx_graph(self):\n",
    "        '''Get dataset as a NetworkX graph.'''\n",
    "        \n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(self.graph[\"collections\"], bipartite=0)\n",
    "        g.add_nodes_from(self.graph[\"tracks\"], bipartite=1) \n",
    "        edge_tuples = [ (e[\"from\"], e[\"to\"]) for e in self.graph[\"edges\"] ] \n",
    "        g.add_edges_from( edge_tuples )\n",
    "\n",
    "        self.track_ids_deg = {i : g.degree[i] for i in self.graph[\"tracks\"]}\n",
    "        self.col_ids_deg = {i : g.degree[i] for i in self.graph[\"collections\"]}\n",
    "\n",
    "        return g#, track_ids_deg, col_ids_deg\n",
    "\n",
    "    def filter_graph(self, g, deg=1):\n",
    "        print(\"Removing nodes with k<={}...\".format(deg))\n",
    "        print(\"Num nodes before filter: {}\".format(len(g.nodes)))\n",
    "        nodes_to_remove = [i for (i, d) in self.track_ids_deg.items() if d <= deg]\n",
    "        g.remove_nodes_from(nodes_to_remove)\n",
    "        # nodes_to_remove = [i for (i, d) in self.track_ids_deg.items() if d >= 51 and d <= 53]\n",
    "        # g.remove_nodes_from(nodes_to_remove)\n",
    "        print(\"Num nodes after filter: {}\".format(len(g.nodes)))\n",
    "        largest_cc = max(nx.connected_components(g), key=len)\n",
    "        print(\"Largest 5 CCs: \", [len(c) for c in sorted(nx.connected_components(g), key=len, reverse=True)][:5])\n",
    "        print(\"Num nodes final: {}\".format(len(largest_cc)))\n",
    "        print(\"Saving new graph...\")\n",
    "        g = g.subgraph(largest_cc)\n",
    "        self.save_graph(g)\n",
    "        return g\n",
    "\n",
    "\n",
    "    def get_playlists_vs_albums(self):\n",
    "        playlist_ids, album_ids = [],[]\n",
    "        for id,info in self.collections.items():\n",
    "            if \"playlist\" in info[\"type\"]:\n",
    "                playlist_ids.append(id)\n",
    "            elif \"album\" in info[\"type\"]:\n",
    "                album_ids.append(id)\n",
    "\n",
    "        return playlist_ids, album_ids\n",
    "    \n",
    "    def get_playlists_by_keywords(self, keywords):\n",
    "        playlist_ids = []\n",
    "\n",
    "        def keywords_in_info(keywords, info):\n",
    "            return True if (any(word in info[\"name\"].lower() for word in keywords) or \\\n",
    "                            any(word in info[\"description\"].lower() for word in keywords)) else False\n",
    "\n",
    "        for id,info in self.collections.items():\n",
    "            if \"playlist\" in info[\"type\"] and keywords_in_info(keywords, info):\n",
    "                playlist_ids.append(id)\n",
    "\n",
    "        return playlist_ids\n",
    "    \n",
    "    def get_projected_graph(self, graph, is_multigraph=False):\n",
    "        nodes_for_projection = [n for n, a in graph.nodes(data=True) if a[\"bipartite\"]==1]\n",
    "        print(\"Projecting on {} nodes\".format(len(nodes_for_projection)))\n",
    "        G_projected = bipartite.projected_graph(graph, nodes_for_projection, multigraph=is_multigraph)\n",
    "        return G_projected\n",
    "    \n",
    "    def get_custom_projected_graph(self, graph, is_multigraph=False):\n",
    "        with open(\"dataset/custom_communities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            comm = json.load(f)\n",
    "        nodes_for_projection, nodes_for_projection_t, nodes_for_projection_c = [],[],[]\n",
    "        for tracks in comm[\"tracks\"].values():\n",
    "            nodes_for_projection_t += tracks\n",
    "        print(\"Tracks: \", len(nodes_for_projection_t))\n",
    "        for col in comm[\"collections\"].values():\n",
    "            nodes_for_projection_c += col\n",
    "        print(\"Collections: \", len(nodes_for_projection_c))\n",
    "\n",
    "        nodes_for_subgraph = nodes_for_projection_t + nodes_for_projection_c\n",
    "        nodes_for_subgraph = list(set(nodes_for_subgraph))\n",
    "        print(\"Nodes w/o duplicates: \",len(nodes_for_subgraph))\n",
    "        g = graph.subgraph(nodes_for_subgraph)\n",
    "        print(\"Total CCs: \", len([len(c) for c in sorted(nx.connected_components(g), key=len, reverse=True)]))\n",
    "        print(\"Largest 5 CCs: \", [len(c) for c in sorted(nx.connected_components(g), key=len, reverse=True)][:5])\n",
    "        print(\"Smallest 5 CCs: \", [len(c) for c in sorted(nx.connected_components(g), key=len, reverse=False)][:5])\n",
    "        \n",
    "        largest_cc = max(nx.connected_components(g), key=len)\n",
    "        G_bipartite = graph.subgraph(largest_cc)\n",
    "        print(list(G_bipartite.nodes(data=True))[:2])\n",
    "        nodes_for_projection_t = [n for n in nodes_for_projection_t if n in G_bipartite.nodes()]\n",
    "        nodes_for_projection_c = [n for n in nodes_for_projection_c if n in G_bipartite.nodes()]\n",
    "        print(\"Projecting on {} nodes\".format(len(set(nodes_for_projection_t))))\n",
    "\n",
    "        G_projected = bipartite.projected_graph(G_bipartite, nodes_for_projection_t, multigraph=is_multigraph)\n",
    "        return G_projected, G_bipartite\n",
    "    \n",
    "    def save_community(self, pred, algo_name):\n",
    "        readwrite.write_community_csv(pred, path.join(self.save_dir, \"{}_communities.csv\".format(algo_name)), \",\")\n",
    "\n",
    "    def find_communities(self, g, algorithm):\n",
    "        algorithm_name = algorithm.__name__\n",
    "        try:\n",
    "            with timeout(60*35, exception=RuntimeError):\n",
    "                print(\"Starting community detection for {} algorithm\".format(algorithm_name))\n",
    "                if algorithm_name == \"angel\":\n",
    "                    community_prediction = algorithm(g, threshold=0.3, min_community_size=5000)\n",
    "                elif algorithm_name == \"node_perception\":\n",
    "                    community_prediction = algorithm(g, threshold=0.3, overlap_threshold=0.3, min_comm_size=5000)\n",
    "                elif algorithm_name == \"CPM_Bipartite\":\n",
    "                    community_prediction = algorithm(g, 0.3)\n",
    "                elif algorithm_name == \"spectral\":\n",
    "                    community_prediction = algorithm(g, kmax=17)\n",
    "                else:\n",
    "                    community_prediction = algorithm(g)\n",
    "                print(\"Saving...\")\n",
    "                self.save_community(community_prediction, algorithm_name)\n",
    "        except Exception as e:\n",
    "            print(\"Error with {} algorithm\".format(algorithm_name))\n",
    "            print(type(e), e)\n",
    "        else:\n",
    "            print(\"Saved communities file for {} algorithm\".format(algorithm_name))\n",
    "\n",
    "    def find_common_keywords(self):\n",
    "        all_keywords = defaultdict(int)\n",
    "        for id, info in tqdm(self.collections.items()):\n",
    "            if \"playlist\" in info[\"type\"]:\n",
    "                if \"<a href=:\" in info[\"description\"]:\n",
    "                    decription = []\n",
    "                    for i in info[\"description\"].split(\", \"):\n",
    "                        decription += i.lower().split(\">\")[1].split(\"</a\")[0].split()\n",
    "                else:\n",
    "                    decription = info[\"description\"].lower()\\\n",
    "                                .replace(\"(\",\"\").replace(\")\",\"\").replace(\"{\",\"\").replace(\"}\",\"\")\\\n",
    "                                .replace(\"[\",\"\").replace(\"]\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\\\n",
    "                                .replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\".\",\"\")\\\n",
    "                                .replace(\"-\",\"\").replace(\"–\",\"\").replace(\";\",\"\").replace(\":\",\"\")\\\n",
    "                                .replace(\"&\",\"\").replace(\"%\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")\\\n",
    "                                .replace(\"$\",\"\").replace(\"|\",\"\").split()\n",
    "\n",
    "                name = info[\"name\"].lower()\\\n",
    "                                .replace(\"(\",\"\").replace(\")\",\"\").replace(\"{\",\"\").replace(\"}\",\"\")\\\n",
    "                                .replace(\"[\",\"\").replace(\"]\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\\\n",
    "                                .replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\".\",\"\")\\\n",
    "                                .replace(\"-\",\"\").replace(\"–\",\"\").replace(\";\",\"\").replace(\":\",\"\")\\\n",
    "                                .replace(\"&\",\"\").replace(\"%\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")\\\n",
    "                                .replace(\"$\",\"\").replace(\"|\",\"\").split()\n",
    "                \n",
    "                for word in name + decription:\n",
    "                    all_keywords[word] += 1\n",
    "        \n",
    "        \n",
    "        with open(path.join(self.base_dir, \"phrases.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict(phrases=dict(sorted(all_keywords.items(), key=lambda item: item[1], reverse=True))), \\\n",
    "                            f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Example usage of the SpotifyGraph dataset class\n",
    "    \n",
    "\n",
    "    # JSON COLLECTIONS STRUCTURE FOR EACH PLAYLIST - example\n",
    "    # \"type\": \"playlist\",\n",
    "    # \"name\": \"Adrenaline Workout\",\n",
    "    # \"num_tracks\": 31,\n",
    "    # \"description\": \"If your workout doubles as an outlet for your aggression\",\n",
    "    # \"ztracks\": [ track ids ]\n",
    "\n",
    "\n",
    "# to je iz hw3 sam sample \n",
    "\n",
    "            # g = girvan_newman_graph(mi)\n",
    "            # louvain = algorithms.louvain(g)\n",
    "            # walktrap = algorithms.walktrap(g)\n",
    "            # label_prop = algorithms.label_propagation(g)\n",
    "            # true_labels = classes.NodeClustering([[3*i + j for i in range(24)] for j in range(3)], g)\n",
    "\n",
    "            # a += evaluation.normalized_mutual_information(true_labels, louvain).score\n",
    "            # b += evaluation.normalized_mutual_information(true_labels, walktrap).score\n",
    "            # c += evaluation.normalized_mutual_information(true_labels, label_prop).score\n",
    "\n",
    "            ##############################################################################\n",
    "\n",
    "            # truth = [[i for i in range(1000)]]\n",
    "            # g = nx.gnm_random_graph(1000, 1000*k)\n",
    "            # true_labels = classes.NodeClustering(truth, g)\n",
    "            # louvain = algorithms.louvain(g)\n",
    "            # walktrap = algorithms.walktrap(g)\n",
    "            # label_prop = algorithms.label_propagation(g)\n",
    "\n",
    "            # a += evaluation.variation_of_information(true_labels, louvain).score\n",
    "            # b += evaluation.variation_of_information(true_labels, walktrap).score\n",
    "            # c += evaluation.variation_of_information(true_labels, label_prop).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph...\n",
      "Num nodes: 1563358\n",
      "Bipartite? True\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "data = SpotifyGraph(root, None)\n",
    "g = data.to_nx_graph()\n",
    "print(\"Num nodes:\", len(g))\n",
    "#data.find_common_keywords()\n",
    "print(\"Bipartite?\", bipartite.is_bipartite(g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already have filtered graph you can skip this\n",
    "#g = data.filter_graph(g, deg=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting projection...\n",
      "1563358 True\n",
      "Tracks:  302136\n",
      "Collections:  12415\n",
      "Nodes w/o duplicates:  262280\n",
      "Total CCs:  679\n",
      "Largest 5 CCs:  [236889, 371, 336, 224, 175]\n",
      "Smallest 5 CCs:  [6, 6, 6, 6, 6]\n",
      "[('7Mj1kQLaqu6Rr6rwAIJQQh', {'bipartite': 1}), ('0wL0kk1N7cGjDpanYOsYo4', {'bipartite': 1})]\n",
      "Projecting on 225264 nodes\n",
      "225264 False\n",
      "236889 True\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting projection...\")\n",
    "#g_ = data.get_projected_graph(g)\n",
    "print(len(g.nodes), bipartite.is_bipartite(g))\n",
    "g_, gb_ = data.get_custom_projected_graph(g)\n",
    "print(len(g_.nodes), bipartite.is_bipartite(g_))\n",
    "print(len(gb_.nodes), bipartite.is_bipartite(gb_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CCs:  1\n",
      "Largest 5 CCs:  [225264]\n"
     ]
    }
   ],
   "source": [
    "largest_cc = max(nx.connected_components(g_), key=len)\n",
    "\n",
    "print(\"Total CCs: \", len([len(c) for c in sorted(nx.connected_components(g_), key=len, reverse=True)]))\n",
    "print(\"Largest 5 CCs: \", [len(c) for c in sorted(nx.connected_components(g_), key=len, reverse=True)][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_overlapping_algorithms = [algorithms.angel,\n",
    "                                  algorithms.core_expansion,\n",
    "                                  algorithms.node_perception,\n",
    "                                  algorithms.lpanni,\n",
    "                                  algorithms.graph_entropy,\n",
    "                                  algorithms.umstmo,\n",
    "\n",
    "                                #   algorithms.lemon,\n",
    "                                #   algorithms.multicom,\n",
    "                                #   algorithms.overlapping_seed_set_expansion,\n",
    "                                  ]\n",
    "list_of_crisp_algorithms = [algorithms.leiden, \n",
    "                            algorithms.infomap, \n",
    "                            algorithms.sbm_dl,\n",
    "                            ]\n",
    "list_of_bipartite_algorithms = [#algorithms.bimlpa, \n",
    "                                algorithms.condor,\n",
    "                                algorithms.CPM_Bipartite,\n",
    "                                #algorithms.infomap_bipartite,\n",
    "                                #algorithms.spectral,\n",
    "                                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting community detection...\n",
      "\n",
      "Starting community detection for condor algorithm\n",
      "Error with condor algorithm\n",
      "<class 'AssertionError'> The network must be bipartite.\n",
      "\n",
      "Starting community detection for CPM_Bipartite algorithm\n",
      "Error with CPM_Bipartite algorithm\n",
      "<class 'ValueError'> invalid literal for int() with base 10: '7Mj1kQLaqu6Rr6rwAIJQQh'\n",
      "\n",
      "Starting community detection for spectral algorithm\n",
      "Error with spectral algorithm\n",
      "<class 'numpy.core._exceptions._ArrayMemoryError'> Unable to allocate 378. GiB for an array with shape (225264, 225264) and data type int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting community detection...\\n\")\n",
    "\n",
    "# for algo in list_of_overlapping_algorithms:\n",
    "#     data.find_communities(g_, algo)\n",
    "#     print()\n",
    "for algo in list_of_bipartite_algorithms:\n",
    "    data.find_communities(gb_, algo)\n",
    "    print()\n",
    "# for algo in list_of_crisp_algorithms:\n",
    "#     data.find_communities(g_, algo)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236889 [('7Mj1kQLaqu6Rr6rwAIJQQh', {'bipartite': 1, 'node_id': '7Mj1kQLaqu6Rr6rwAIJQQh'}), ('0wL0kk1N7cGjDpanYOsYo4', {'bipartite': 1, 'node_id': '0wL0kk1N7cGjDpanYOsYo4'})]\n",
      "236889 [(0, {'bipartite': 1, 'node_id': '7Mj1kQLaqu6Rr6rwAIJQQh'}), (1, {'bipartite': 1, 'node_id': '0wL0kk1N7cGjDpanYOsYo4'})]\n",
      "Starting community detection for condor algorithm\n",
      "Error with condor algorithm\n",
      "<class 'AssertionError'> The network must be bipartite.\n",
      "\n",
      "Starting community detection for CPM_Bipartite algorithm\n",
      "Saving...\n",
      "Saved communities file for CPM_Bipartite algorithm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "for i, (n, a) in enumerate(gb_.nodes(data=True)):\n",
    "    a[\"node_id\"] = n\n",
    "    mapping[n] = i\n",
    "gb_relabeled = nx.relabel_nodes(gb_, mapping)\n",
    "print(len(gb_.nodes(data=True)), list(gb_.nodes(data=True))[:2])\n",
    "print(len(gb_relabeled.nodes(data=True)), list(gb_relabeled.nodes(data=True))[:2])\n",
    "\n",
    "for algo in list_of_bipartite_algorithms:\n",
    "    data.find_communities(gb_relabeled, algo)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # GT_IDS for evaluation after community detection\n",
    "    #playlist_ids, album_ids = dataset.get_playlists_vs_albums()\n",
    "\n",
    "\n",
    "    # hand picked filter words that occour in name or description of the playlists\n",
    "    #keywords = [\"fitness\", \"workout\"]       \n",
    "    #selected_ids = dataset.get_playlists_by_keywords(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "481f671b25a3de7ce29310ade63732b5e6b538537deca2ea013019a2a265f36c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
